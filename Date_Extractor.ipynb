{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Date_attention_thingy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "PNdco5R_37N2",
        "Uao-4-B44SAO",
        "R0jbOn2T4n6D",
        "p13uJx9exD94",
        "eHVHM5-LmYZD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "PNdco5R_37N2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Colab"
      ]
    },
    {
      "metadata": {
        "id": "uYHgxPiedr15",
        "colab_type": "code",
        "outputId": "c08fe9fc-20ae-49ea-9b34-3f5665bd80a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4.0\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6WXSslxueGlC",
        "colab_type": "code",
        "outputId": "7afb1831-a261-4f1a-d48e-72c6a662c8b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install Pillow==4.0.0\n",
        "!pip3 install PIL\n",
        "!pip3 install image"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow==4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/e8/b3fbf87b0188d22246678f8cd61e23e31caa1769ebc06f1664e2e5fe8a17/Pillow-4.0.0-cp36-cp36m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.6MB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mtorchvision 0.2.1 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 5.3.0\n",
            "    Uninstalling Pillow-5.3.0:\n",
            "      Successfully uninstalled Pillow-5.3.0\n",
            "Successfully installed Pillow-4.0.0\n",
            "Collecting PIL\n",
            "\u001b[31m  Could not find a version that satisfies the requirement PIL (from versions: )\u001b[0m\n",
            "\u001b[31mNo matching distribution found for PIL\u001b[0m\n",
            "Requirement already satisfied: image in /usr/local/lib/python3.6/dist-packages (1.5.27)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from image) (4.0.0)\n",
            "Requirement already satisfied: django in /usr/local/lib/python3.6/dist-packages (from image) (2.1.4)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->image) (0.46)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from django->image) (2018.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zfvjddlFeIxv",
        "colab_type": "code",
        "outputId": "21b66d00-f9f7-4f03-84bd-8c941aba7665",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "import numpy as np\n",
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4.0\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Uao-4-B44SAO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generating dataset"
      ]
    },
    {
      "metadata": {
        "id": "u6jh-oiQgaJ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install faker\n",
        "!pip3 install babel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iLmsDfvvgcx7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from date_utils import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CmudfDAdgre7",
        "colab_type": "code",
        "outputId": "298915fd-c6e1-4e71-854d-73a253c1060e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  x=load_date()\n",
        "  print(\"Input (human-readable): \"+str(x[0])+\"\\nTarget (machine-readable): \"+str(x[1])+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input (human-readable): 9 may 1998\n",
            "Target (machine-readable): 1998-05-09\n",
            "\n",
            "Input (human-readable): 10.09.70\n",
            "Target (machine-readable): 1970-09-10\n",
            "\n",
            "Input (human-readable): 4/28/90\n",
            "Target (machine-readable): 1990-04-28\n",
            "\n",
            "Input (human-readable): thursday january 26 1995\n",
            "Target (machine-readable): 1995-01-26\n",
            "\n",
            "Input (human-readable): monday march 7 1983\n",
            "Target (machine-readable): 1983-03-07\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GLoAi3Kdgwfu",
        "colab_type": "code",
        "outputId": "0bc9e85b-9bee-478a-841e-2c02e187909e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "m=10000 #Size of sample database\n",
        "dataset,human_vocab,machine_vocab,inv_machine=load_dataset(m)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 19382.69it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "tIDQTU_xw3td",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset[:3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZTHjE0Rjxdm6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Converting inputs/targets from string to list of onehot-encoded chars\n",
        "X,Y,X_oh,Y_oh=preprocess_data(dataset,human_vocab,machine_vocab,30,10)\n",
        "#All inputs are converted to len=30 by adding '<pad>' element if necessary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wQ3M193zyK8M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Original\\n\"+str(dataset[5])+\"\\nConverted to ints\\n\"+\n",
        "      str(X[5]) + \" , \" + str(Y[5])+\"\\nOne-hotted\\n\"+\n",
        "      str(X_oh[5]) + \" , \" + str(Y_oh[5])+\"\\n\")      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R0jbOn2T4n6D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "liEI7Z7b11Gu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_oh = X_oh.reshape([30,10000,37])\n",
        "X_oh = torch.tensor(X_oh)\n",
        "Y    = torch.tensor(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ghm8QT_EH3sn",
        "colab_type": "code",
        "outputId": "9badfbdc-5f80-4b9e-8f00-7d6a032c8bb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"X_oh shape: \" + str(X_oh.shape)+\"\\nY shape:\"+str(Y.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_oh shape: torch.Size([30, 10000, 37])\n",
            "Y_oh shape:torch.Size([10, 10000, 11])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d4tryHkd4nCQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Context(nn.Module):\n",
        "  \"\"\"\n",
        "  Computes context vector from prev. hidden state [1x100x64] and activations [30x100x64]\n",
        "  \n",
        "  \"\"\"\n",
        "  def __init__(self,input_size,hidden_size): #hidden_size -> 10\n",
        "    super(Context, self).__init__()   \n",
        "    self.i2h = nn.Linear(input_size,hidden_size)\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.h2o = nn.Linear(hidden_size,1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.softmax = nn.Softmax(0) #dimension\n",
        "    \n",
        "  def forward(self, act, hid):\n",
        "    \"\"\"\n",
        "    activation vector from pre-attention LSTM [30 x 100 x 64]\n",
        "    previous hidden state of post-att. LSTM [1 x 100 x 64]\n",
        "    \n",
        "    \"\"\"  \n",
        "    output = torch.cat((act,hid.expand([30,-1,64])),2) #concatenating act & hid\n",
        "    output = self.tanh(self.i2h(output))                       #[30 x 100 x hidden_size]\n",
        "    output = self.relu(self.h2o(output))                       #[30 x 100 x 1]\n",
        "    output = self.softmax(output)\n",
        "    output = act * output\n",
        "    output = torch.sum(output,0)\n",
        "    output = output.view(1,-1,64)\n",
        "    return output                                              #[1 x 100 x 64]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RTCggbdJT1m2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class post_LSTM(nn.Module):\n",
        "  \"\"\"\n",
        "  Computes 1 output from the current context vector.\n",
        "  h - LSTM hidden state\n",
        "  c - LSTM memory unit\n",
        "  \n",
        "  \"\"\"\n",
        "  def __init__(self, ip_size, hid_size, op_size):\n",
        "    super(post_LSTM, self).__init__()\n",
        "    self.post_LSTM = nn.LSTM(input_size=ip_size,hidden_size=hid_size)\n",
        "    self.o2o = nn.Linear(hid_size,op_size)\n",
        "    self.softmax = nn.Softmax(1) #dimension\n",
        "    \n",
        "  def forward(self, context, h, c):\n",
        "    #context -> [1 x 100 x ip_size]\n",
        "    output, (h,c) = self.post_LSTM(context,(h,c))\n",
        "    output = self.o2o(output)                       #[1 x 100 x hid_size]\n",
        "    output = self.softmax(output)\n",
        "    return output,(h,c)                             #[1 x 100 x op_size], h-> [1 x 100 x hid_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fvblJrN7LCiW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pre_BiLSTM = nn.LSTM(input_size=37, hidden_size=32,bidirectional=True)        #input  -> [30 x 100 x 37]   #output -> [30 x 100 x 64]\n",
        "cxt_model = Context(128,10)\n",
        "post_model = post_LSTM(64,64,11)\n",
        "hs = torch.zeros([1,100,64]) #hidden state\n",
        "mu = torch.zeros([1,100,64]) #memory unit\n",
        "batch_output = torch.zeros([10,100,11])\n",
        "loss_func = nn.NLLLoss()\n",
        "optimizer1 = optim.Adam(params = pre_BiLSTM.parameters(),lr = 0.005,weight_decay = 0.01)\n",
        "optimizer2 = optim.Adam(params = cxt_model.parameters(),lr = 0.005,weight_decay = 0.01)\n",
        "optimizer3 = optim.Adam(params = post_model.parameters(),lr = 0.005,weight_decay = 0.01)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bl5zWYo4IbgP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in (range(100)): #for each batch\n",
        "  loss=0\n",
        "  optimizer1.zero_grad()\n",
        "  optimizer2.zero_grad()\n",
        "  optimizer3.zero_grad()\n",
        "  \n",
        "  ii = i*100\n",
        "  \n",
        "  activations,(_,_) = pre_BiLSTM(X_oh[:,ii : ii + 100,:])\n",
        "  \n",
        "  for t in range(10):\n",
        "    context = cxt_model(activations, hs)\n",
        "    out,(hs,mu) = post_model(context,hs,mu)\n",
        "    \n",
        "    loss+=loss_func(out.view([100,11]),Y[ii:ii+100,t])\n",
        "  loss.backward(retain_graph=True)\n",
        "  optimizer1.step()\n",
        "  optimizer2.step()\n",
        "  optimizer3.step()\n",
        "  \n",
        "  if(i%10==0):\n",
        "    print(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p13uJx9exD94",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model analysis"
      ]
    },
    {
      "metadata": {
        "id": "TT22zUAHoAdj",
        "colab_type": "code",
        "outputId": "4cb20f8e-5c71-4dde-b825-5775c9b04efe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(pre_BiLSTM)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM(37, 32, bidirectional=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7U4yqv_2vwYh",
        "colab_type": "code",
        "outputId": "80859347-9d4e-4344-f40d-2a9ab8c10efe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "print(cxt_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Context(\n",
            "  (i2h): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (tanh): Tanh()\n",
            "  (h2o): Linear(in_features=10, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (softmax): Softmax()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tcWXfRgOvz5T",
        "colab_type": "code",
        "outputId": "1b437d3e-c9c9-409e-8ba6-cd6ae2bd098e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "cell_type": "code",
      "source": [
        "print(post_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "post_LSTM(\n",
            "  (post_LSTM): LSTM(64, 64)\n",
            "  (o2o): Linear(in_features=64, out_features=11, bias=True)\n",
            "  (softmax): Softmax()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AGKioA841a47",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P6Avx_pZ1t5t",
        "colab_type": "code",
        "outputId": "c806f14a-b5f6-4b06-c653-0c42361c9669",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "count_parameters(pre_BiLSTM) + count_parameters(post_model) + count_parameters(cxt_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53472"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "eHVHM5-LmYZD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing\n"
      ]
    },
    {
      "metadata": {
        "id": "dMSAzVwHmbKC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(inp): #[]\n",
        "  answer = ''\n",
        "  with torch.no_grad():\n",
        "    _,_,inp_oh,_=preprocess_data([inp,'0000000000'],human_vocab,machine_vocab,30,10)\n",
        "    inp_oh = torch.tensor(inp_oh.view(30,1,37))\n",
        "    \n",
        "    activations,(_,_) = pre_BiLSTM(inp_oh)\n",
        "  \n",
        "    for t in range(10):\n",
        "      context = cxt_model(activations, hs)\n",
        "      out,(hs,mu) = post_model(context,hs,mu)\n",
        "      out = out.view([1,-1])\n",
        "      _,ans = torch.max(out,1)\n",
        "      answer += inv_machine[int(ans)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TviWiz35wJHn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gBzgJ3kIv4hc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# softmax axis dimension in context\n",
        "# retain_graph"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
